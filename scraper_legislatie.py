#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Script principal pentru parsarea automatÄƒ a actelor legislative
CiteÈ™te linkuri din fiÈ™ier È™i salveazÄƒ rezultatele Ã®n format Excel
"""

import os
import re
import time
import requests
from urllib.parse import urlparse
from datetime import datetime
from typing import List, Optional
import pandas as pd
from bs4 import BeautifulSoup

from hybrid_parser import HybridLegislativeParser

class LegislationScraper:
    def __init__(self, links_file: str = "linkuri_legislatie.txt", output_dir: str = "rezultate"):
        """
        IniÈ›ializeazÄƒ scraper-ul pentru legislaÈ›ie
        
        Args:
            links_file: Calea cÄƒtre fiÈ™ierul cu linkuri
            output_dir: Directorul unde se salveazÄƒ rezultatele
        """
        self.links_file = links_file
        self.output_dir = output_dir
        self.session = requests.Session()
        
        # Headers pentru a simula un browser real
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ro-RO,ro;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # CreazÄƒ directorul de output dacÄƒ nu existÄƒ
        os.makedirs(self.output_dir, exist_ok=True)
    
    def read_links(self) -> List[str]:
        """
        CiteÈ™te linkurile din fiÈ™ier, ignorÃ¢nd comentariile È™i liniile goale
        
        Returns:
            Lista de linkuri URL
        """
        links = []
        
        if not os.path.exists(self.links_file):
            print(f"âŒ FiÈ™ierul {self.links_file} nu existÄƒ!")
            return links
        
        with open(self.links_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                
                # IgnorÄƒ liniile goale È™i comentariile
                if not line or line.startswith('#'):
                    continue
                
                # ValideazÄƒ cÄƒ este un URL valid
                if self.is_valid_url(line):
                    links.append(line)
                else:
                    print(f"âš ï¸  Linia {line_num}: URL invalid ignorat: {line}")
        
        print(f"ğŸ“‹ Am gÄƒsit {len(links)} linkuri valide Ã®n {self.links_file}")
        return links
    
    def is_valid_url(self, url: str) -> bool:
        """
        VerificÄƒ dacÄƒ URL-ul este valid È™i de la legislatie.just.ro
        
        Args:
            url: URL-ul de verificat
            
        Returns:
            True dacÄƒ URL-ul este valid
        """
        try:
            result = urlparse(url)
            return (result.scheme in ['http', 'https'] and 
                   'legislatie.just.ro' in result.netloc)
        except Exception:
            return False
    
    def get_printable_url(self, base_url: str) -> str:
        """
        ConverteÈ™te URL-ul de bazÄƒ Ã®n URL pentru forma printabilÄƒ
        
        Args:
            base_url: URL-ul original
            
        Returns:
            URL-ul pentru forma printabilÄƒ
        """
        # DacÄƒ deja conÈ›ine 'FormaConsolidata', returneazÄƒ cum este
        if 'FormaConsolidata' in base_url:
            return base_url
        
        # Altfel, Ã®ncearcÄƒ sÄƒ construiascÄƒ URL-ul pentru forma printabilÄƒ
        if 'DetaliiDocument' in base_url:
            # Extrage ID-ul documentului
            match = re.search(r'DetaliiDocument/(\d+)', base_url)
            if match:
                doc_id = match.group(1)
                return f"https://legislatie.just.ro/Public/FormaConsolidata/{doc_id}"
        
        return base_url
    
    def download_document(self, url: str) -> Optional[str]:
        """
        DescarcÄƒ conÈ›inutul documentului de la URL
        
        Args:
            url: URL-ul documentului
            
        Returns:
            ConÈ›inutul textual al documentului sau None dacÄƒ a eÈ™uat
        """
        try:
            print(f"ğŸ“¥ Descarc: {url}")
            
            # ÃncearcÄƒ mai multe strategii, prioritizÃ¢nd FormaPrintabila (versiune curatÄƒ)
            urls_to_try = []
            
            # DetecteazÄƒ È™i converteÈ™te URL-ul cÄƒtre FormaPrintabila
            if 'FormaPrintabila' in url:
                # Deja e FormaPrintabila, foloseÈ™te direct
                urls_to_try.append(url)
            elif 'DetaliiDocument' in url or 'FormaConsolidata' in url:
                # Extrage ID-ul (poate fi numeric sau alfanumeric)
                match = re.search(r'(?:DetaliiDocument|FormaConsolidata)/([A-Z0-9]+)', url)
                if match:
                    doc_id = match.group(1)
                    # PRIORITATE: FormaPrintabila (fÄƒrÄƒ linkuri cÄƒtre alte acte)
                    urls_to_try.extend([
                        f"https://legislatie.just.ro/Public/FormaPrintabila/{doc_id}",
                        f"https://legislatie.just.ro/Public/FormaConsolidata/{doc_id}",
                        url  # URL-ul original ca fallback
                    ])
                else:
                    urls_to_try.append(url)
            else:
                urls_to_try.append(url)
            
            last_error = None
            for attempt_url in urls_to_try:
                try:
                    if attempt_url != url:
                        print(f"   â†’ Ãncerc: {attempt_url}")
                    
                    # ConfigureazÄƒ sesiunea pentru aceastÄƒ cerere
                    response = self.session.get(
                        attempt_url, 
                        timeout=30, 
                        allow_redirects=True
                    )
                    response.raise_for_status()
                    
                    # VerificÄƒ dacÄƒ am primit conÈ›inut util
                    if len(response.text) < 100:
                        print(f"   âš ï¸  RÄƒspuns prea scurt ({len(response.text)} caractere)")
                        continue
                    
                    # ÃncearcÄƒ sÄƒ detecteze encoding-ul
                    if response.encoding is None:
                        response.encoding = 'utf-8'
                    
                    print(f"   âœ… Succes cu {attempt_url}")
                    print(f"   ğŸ“„ Dimensiune conÈ›inut: {len(response.text)} caractere")
                    
                    return response.text
                    
                except requests.exceptions.TooManyRedirects:
                    last_error = f"Prea multe redirecturi pentru {attempt_url}"
                    print(f"   âŒ {last_error}")
                    continue
                except requests.exceptions.RequestException as e:
                    last_error = f"Eroare cerere pentru {attempt_url}: {e}"
                    print(f"   âŒ {last_error}")
                    continue
            
            print(f"âŒ Toate Ã®ncercÄƒrile au eÈ™uat. Ultima eroare: {last_error}")
            return None
            
        except Exception as e:
            print(f"âŒ Eroare neaÈ™teptatÄƒ pentru {url}: {e}")
            return None
    
    def extract_text_from_html(self, html_content: str) -> Optional[str]:
        """
        Extrage textul curat din conÈ›inutul HTML
        
        Args:
            html_content: ConÈ›inutul HTML
            
        Returns:
            Textul curat sau None dacÄƒ nu se poate extrage
        """
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ÃncearcÄƒ sÄƒ gÄƒseascÄƒ containerul principal cu conÈ›inutul
            # CautÄƒ diverse clase/id-uri comune pentru conÈ›inutul legislativ
            content_selectors = [
                '#content',
                '.content',
                '#main-content',
                '.main-content',
                '.document-content',
                '.law-content',
                '.act-content',
                'main',
                '.container',
                'body'
            ]
            
            text_content = None
            for selector in content_selectors:
                element = soup.select_one(selector)
                if element:
                    # EliminÄƒ scripturile È™i stilurile
                    for script in element(["script", "style", "nav", "header", "footer"]):
                        script.decompose()
                    
                    text_content = element.get_text(separator='\n', strip=True)
                    if len(text_content) > 1000:  # SÄƒ fie suficient de substanÈ›ial
                        break
            
            if not text_content:
                # Fallback: extrage tot textul din body
                text_content = soup.get_text(separator='\n', strip=True)
            
            if text_content and len(text_content) > 500:
                return text_content
            
            return None
            
        except Exception as e:
            print(f"âš ï¸  Eroare la extragerea textului din HTML: {e}")
            return None
    
    def extract_document_id(self, url: str) -> str:
        """
        Extrage ID-ul documentului din URL pentru nume fiÈ™ier
        
        Args:
            url: URL-ul documentului
            
        Returns:
            ID-ul documentului sau timestamp dacÄƒ nu poate fi extras
        """
        match = re.search(r'(?:DetaliiDocument|FormaConsolidata)/(\d+)', url)
        if match:
            return match.group(1)
        
        # Fallback: foloseÈ™te timestamp
        return datetime.now().strftime("%Y%m%d_%H%M%S")
    
    def process_document(self, url: str, index: int, total: int) -> bool:
        """
        ProceseazÄƒ un singur document: descarcÄƒ, parseazÄƒ È™i salveazÄƒ
        
        Args:
            url: URL-ul documentului
            index: Indexul curent (pentru progres)
            total: NumÄƒrul total de documente
            
        Returns:
            True dacÄƒ procesarea a avut succes
        """
        print(f"\nğŸ”„ Procesez documentul {index}/{total}")
        
        # DescarcÄƒ conÈ›inutul
        content = self.download_document(url)
        if not content:
            return False
        
        try:
            # FoloseÈ™te parserul hibrid nou (simplificat È™i robust)
            print("âš™ï¸  Parsez conÈ›inutul...")
            parser = HybridLegislativeParser()
            df, metrics = parser.parse(content, content_type='html')
            
            if df.empty:
                print("âš ï¸  Nu am gÄƒsit date parsabile Ã®n document")
                
                # SalveazÄƒ conÈ›inutul pentru debugging
                doc_id = self.extract_document_id(url)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                debug_file = os.path.join(self.output_dir, f"debug_content_{timestamp}.txt")
                with open(debug_file, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"ğŸ› ConÈ›inut complet salvat Ã®n: debug_content_{timestamp}.txt")
                return False
            
            # GenereazÄƒ numele fiÈ™ierului
            doc_id = self.extract_document_id(url)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # SalveazÄƒ folosind metoda din hybrid_parser (CSV + Markdown)
            saved_files = parser.save_to_rezultate(df, f"act_legislativ_{doc_id}", metrics)
            
            if saved_files.get('csv'):
                csv_filename = os.path.basename(saved_files['csv'])
                print(f"âœ… Salvat CSV: {csv_filename}")
            if saved_files.get('markdown'):
                md_filename = os.path.basename(saved_files['markdown'])
                print(f"âœ… Salvat MD:  {md_filename}")
            print(f"   ğŸ“Š {len(df)} articole procesate")
            print(f"   ğŸ¯ Confidence: {metrics['confidence']:.2f}")
            
            # AfiÈ™eazÄƒ informaÈ›ii sumare
            if not df.empty and 'tip_act' in df.columns:
                first_row = df.iloc[0]
                if first_row.get('tip_act'):
                    nr = first_row.get('nr_act', '')
                    data = first_row.get('data_an', '')
                    print(f"   ğŸ“„ {first_row['tip_act']} nr. {nr}/{data}")
                if first_row.get('titlu_act'):
                    print(f"   ğŸ“ {first_row['titlu_act'][:80]}...")
            
            return True
            
        except Exception as e:
            print(f"âŒ Eroare la procesarea documentului: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def run(self, delay_seconds: float = 2.0) -> None:
        """
        RuleazÄƒ procesarea pentru toate linkurile din fiÈ™ier
        
        Args:
            delay_seconds: Paiza Ã®ntre cereri (pentru a fi respectuoÈ™i cu serverul)
        """
        print("ğŸš€ Ãncep procesarea actelor legislative...")
        print(f"ğŸ“ Rezultatele vor fi salvate Ã®n: {os.path.abspath(self.output_dir)}")
        print("=" * 70)
        
        # CiteÈ™te linkurile
        links = self.read_links()
        if not links:
            print("âŒ Nu am gÄƒsit linkuri valide pentru procesare!")
            return
        
        # ProceseazÄƒ fiecare link
        successful = 0
        failed = 0
        
        for i, url in enumerate(links, 1):
            success = self.process_document(url, i, len(links))
            
            if success:
                successful += 1
            else:
                failed += 1
            
            # PauzÄƒ Ã®ntre cereri (doar dacÄƒ nu este ultimul)
            if i < len(links):
                print(f"â³ PauzÄƒ {delay_seconds} secunde...")
                time.sleep(delay_seconds)
        
        # AfiÈ™eazÄƒ rezultatele finale
        print("\n" + "=" * 70)
        print("ğŸ Procesare finalizatÄƒ!")
        print(f"âœ… Succes: {successful} documente")
        print(f"âŒ EÈ™ec: {failed} documente")
        print(f"ğŸ“ FiÈ™ierele sunt salvate Ã®n: {os.path.abspath(self.output_dir)}")


def main():
    """FuncÈ›ia principalÄƒ"""
    print("ğŸ›ï¸  Scraper pentru Acte Legislative")
    print("=" * 50)
    
    # IniÈ›ializeazÄƒ scraper-ul
    scraper = LegislationScraper()
    
    # RuleazÄƒ procesarea
    scraper.run(delay_seconds=2.0)


if __name__ == "__main__":
    main()